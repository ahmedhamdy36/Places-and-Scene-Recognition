# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y85h3O9v4yaVpkmu6CTvNjb1nxP8z9or
"""

pip install tflearn

import cv2
import numpy as np
import os
from random import shuffle
import csv
import tflearn
from tflearn.layers.conv import *
from tflearn.layers.core import input_data, dropout, fully_connected, flatten
from tflearn.layers.estimator import regression

from tflearn.layers.merge_ops import merge

from google.colab import drive
drive.mount('/content/drive')

IMG_SIZE = 224
train_path = '/content/drive/MyDrive/train_images/train_images/'
test_path = '/content/drive/MyDrive/test_imgs/test_images/'
csv_path='/content/drive/MyDrive/train.csv'

def create_label(image_name):
    label = np.zeros(6)
    with open(csv_path, "r") as sheet:
        r = csv.reader(sheet)
        for i in r:
            if(i[0] == image_name):
              label[int(i[1])] = 1
              break
    return label

def create_train_data():
    training_data = []
    for img in os.listdir(train_path):
        path = os.path.join(train_path, img)
        img_data = cv2.imread(path)
        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))
        training_data.append([(np.array(img_data)), create_label(img)])
    shuffle(training_data)
    np.save('train_data.npy', training_data)
    return training_data

if (os.path.exists('train_data.npy')): 
    train_data =np.load('train_data.npy',allow_pickle=True)
else: 
    train_data = create_train_data()

train = train_data
X_train = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE,3)
y_train = [i[1] for i in train]
conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 3], name='input')
conv1 = conv_2d(conv_input, 32, 3, strides=2, activation='relu',padding="valid")
conv2 = conv_2d(conv1, 32,  3,  strides=1,activation='relu',padding="valid")
conv3 = conv_2d(conv2, 32,  3, strides=1 , activation='relu',padding="same")


pool1 = max_pool_2d(conv3,  kernel_size=3, strides=2,padding="same")
########################################
conv4 = conv_2d(pool1, 80,  1,1, activation='relu',padding="valid")
conv5 = conv_2d(conv4, 192,  1,1, activation='relu',padding="valid")

pool2 = max_pool_2d(conv5, kernel_size=3, strides=2,padding="same")

conv6 = conv_2d(pool2, 32, 1, activation='relu',padding="same",name='conv6')

conv7 = conv_2d(pool2, 32,  1, activation='relu',padding="same")
conv8 = conv_2d(conv7, 32,  3, activation='relu',padding="same")
conv9 = conv_2d(conv8, 32,  3, activation='relu',padding="same",name='conv9')


conv10 = conv_2d(pool2, 32, 1, activation='relu',padding="valid")
conv11 = conv_2d(conv10, 32,  3, activation='relu')
conv12 = conv_2d(conv11, 32,  3, activation='relu',padding="same")
conv13 = conv_2d(conv12, 32,  3, activation='relu',padding="same")
conv14 = conv_2d(conv13, 32,  3, activation='relu',padding="same",name='conv14')

pool3 = avg_pool_2d(pool2,  kernel_size=3, strides=1,padding="same")
conv15 = conv_2d(pool3, 32, 1, activation='relu',padding="valid",name='conv15')

c1 = merge([conv6,conv9,conv14,conv15],mode='concat',axis=3)

###########################

pool4 = max_pool_2d(c1,  kernel_size=3, strides=1,padding="same")
conv16 = conv_2d(c1, 32, 1, activation='relu',padding="same")
conv160 = conv_2d(pool4, 32,  3, activation='relu',padding="same")


conv17 = conv_2d(c1, 32, 1, activation='relu')
conv18 = conv_2d(conv17, 32, 3, activation='relu',padding="same")
conv19 = conv_2d(conv18, 32,  3, activation='relu',padding="same")

c2=merge([conv16,conv19,conv160],mode='concat',axis=3)
# mixed 1: 35 x 35 x 256
########################
conv20 = conv_2d(c2, 32, 1, activation='relu',)

conv21 = conv_2d(c2, 32,  1, activation='relu')
conv22 = conv_2d(conv21, 32,  3, activation='relu',)
conv23 = conv_2d(conv22, 32,  3, activation='relu')



conv24 = conv_2d(c2, 32,  1, activation='relu')
conv25 = conv_2d(conv24, 32,  3, activation='relu')
conv26 = conv_2d(conv25, 32,  3, activation='relu')
conv27 = conv_2d(conv26, 32,  3, activation='relu')
conv28 = conv_2d(conv27, 32,  3, activation='relu')


pool5 = avg_pool_2d(c2,  kernel_size=3, strides=1,padding="same")
conv29 = conv_2d(pool5, 32, 1, activation='relu')

c3=merge([conv20,conv23,conv28,conv29],mode='concat',axis=3)
 # mixed 2: 35 x 35 x 256

##############################
conv30 = conv_2d(c3, 32,  1, activation='relu')

conv300 = conv_2d(conv30, 32,  3, activation='relu')

conv31 = conv_2d(c3, 32,1, activation='relu')
conv32 = conv_2d(conv31, 32, 3, activation='relu')
conv33 = conv_2d(conv32, 32, 3, activation='relu',)
conv34 = conv_2d(conv33, 32,3, activation='relu')

pool6 = max_pool_2d(c3,  kernel_size=3, strides=1,padding="same")

c4=merge([conv300,conv34,pool6],mode='concat',axis=3)


conv35 = conv_2d(c4, 32, 1, activation='relu')

conv36 = conv_2d(c4, 32, 1, activation='relu')
conv37 = conv_2d(conv36, 32, 3, activation='relu')
conv38 = conv_2d(conv36, 32, 3, activation='relu')

c5=merge([conv37,conv38],mode='concat',axis=3)

conv39 = conv_2d(c4, 32, 1, activation='relu')
conv40 = conv_2d(conv39, 32, 3, activation='relu')
conv41 = conv_2d(conv40, 32, 3, activation='relu')
conv42 = conv_2d(conv40, 32, 3, activation='relu')

c6=merge([conv41,conv42],mode='concat',axis=3)

pool7 = avg_pool_2d(c4,  kernel_size=3, strides=1,padding="same")
conv43 = conv_2d(pool7, 32, 1, activation='relu')


c7=merge([conv35,c5,c6,conv43],mode='concat',axis=3)


pool8 = avg_pool_2d(c7,  kernel_size=3, strides=3,padding="same")

fully_layer = dropout(pool8, 0.2)
#fully_layer =flatten(fully_layer)

fully_layer = fully_connected(fully_layer, 512, activation='relu')


cnn_layers = fully_connected(fully_layer, 6, activation='softmax')


cnn_layers = regression(cnn_layers, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy', name='targets')
model = tflearn.DNN(cnn_layers)



#if (os.path.exists('model.tfl.meta')):
  #  model.load('./model.tfl')
#else:
model.fit({'input': X_train}, {'targets': y_train},validation_set=0.18,n_epoch=20,snapshot_step=500, show_metric=True)
  #  model.save('model.tfl')

#model.fit( x = X_train[:12220] , y = np.array(y_train[:12220]) ,epochs = 3,validation_steps= 1,validation_data=[X_train[:-12220],y_train[:-12220]],callbacks=[checkpoint,earlystop])  
model.save('model.tfl')

testing_data = []
for img in os.listdir(test_path):
    path = os.path.join(test_path, img)
    img_data = cv2.imread(path)
    img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))
    test_img = img_data.reshape(IMG_SIZE, IMG_SIZE, 3)
    prediction = model.predict([test_img])[0]
    max_val = max(prediction)
    l = 0
    for i in range(6):
         if prediction[i] == max_val:
            l=i
            break
    testing_data.append([img,l])

h = ['image_name', 'label']
with open('data.csv', 'a') as file:
    writer = csv.writer(file)
    writer.writerow(h)
    for i in range(len(testing_data)):
        writer.writerow(testing_data[i])